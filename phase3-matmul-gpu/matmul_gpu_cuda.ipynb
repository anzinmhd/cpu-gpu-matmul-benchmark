{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkOrbuKaI_Q5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9cb28a-2feb-40d9-bf17-fe60a910fad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul_gpu.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matmul_gpu.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_fp16.h>\n",
        "#include <mma.h>\n",
        "\n",
        "using namespace std;\n",
        "using namespace nvcuda;\n",
        "\n",
        "// USER CONFIGURATION\n",
        "\n",
        "// Matrix Size (Must be a multiple of 16)\n",
        "#define N 8192\n",
        "#define BLOCK_SIZE 32\n",
        "#define DATA_TYPE float\n",
        "\n",
        "// 1. DATA GENERATION & HELPERS\n",
        "\n",
        "// Templated Generator: Works for int, float, and double\n",
        "template<typename T>\n",
        "vector<T> createMatrix(int rows, int cols, unsigned int seed) {\n",
        "    vector<T> M((size_t)rows * cols);\n",
        "    srand(seed);\n",
        "    for (size_t i = 0; i < M.size(); i++) {\n",
        "        // Generate random 0.0 to 1.0\n",
        "        double r = static_cast<double>(rand()) / static_cast<double>(RAND_MAX);\n",
        "        // Scale to -5.0 to 5.0 and cast to target type\n",
        "        M[i] = static_cast<T>((r * 10.0) - 5.0);\n",
        "    }\n",
        "    return M;\n",
        "}\n",
        "\n",
        "// Helper: Converts ANY type (int/float/double) to Half Precision\n",
        "// This allows us to run the Tensor Core benchmark regardless of what DATA_TYPE you chose.\n",
        "template <typename T>\n",
        "vector<half> convertToHalf(const vector<T>& input) {\n",
        "    vector<half> output(input.size());\n",
        "    for(size_t i=0; i<input.size(); i++) {\n",
        "        // Cast input to float first, then convert to half\n",
        "        output[i] = __float2half(static_cast<float>(input[i]));\n",
        "    }\n",
        "    return output;\n",
        "}\n",
        "\n",
        "// 2. STANDARD KERNELS (Templated for DATA_TYPE)\n",
        "\n",
        "// ALGORITHM 1: NAIVE (Global Memory)\n",
        "template <typename T>\n",
        "__global__ void matmul_naive(const T *A, const T *B, T *C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        T sum = 0;\n",
        "        for (int k = 0; k < n; k++) {\n",
        "            sum += A[row * n + k] * B[k * n + col];\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// ALGORITHM 2: TILED (Shared Memory)\n",
        "template <typename T>\n",
        "__global__ void matmul_tiled(const T *A, const T *B, T *C, int n) {\n",
        "    __shared__ T As[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ T Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    T sum = 0;\n",
        "\n",
        "    for (int t = 0; t < (n / BLOCK_SIZE); ++t) {\n",
        "        As[threadIdx.y][threadIdx.x] = A[row * n + (t * BLOCK_SIZE + threadIdx.x)];\n",
        "        Bs[threadIdx.y][threadIdx.x] = B[(t * BLOCK_SIZE + threadIdx.y) * n + col];\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < BLOCK_SIZE; ++k) {\n",
        "            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (row < n && col < n) C[row * n + col] = sum;\n",
        "}\n",
        "\n",
        "\n",
        "// 3. TENSOR CORE KERNEL (Specialized for FP16)\n",
        "\n",
        "// ALGORITHM 3: WMMA (Hardware Acceleration)\n",
        "const int WMMA_M = 16;\n",
        "const int WMMA_N = 16;\n",
        "const int WMMA_K = 16;\n",
        "\n",
        "__global__ void matmul_tensor_core(const half *A, const half *B, float *C, int n) {\n",
        "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n",
        "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> b_frag;\n",
        "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n",
        "\n",
        "    wmma::fill_fragment(c_frag, 0.0f);\n",
        "\n",
        "    int globalWarpM = (blockIdx.y * blockDim.y + threadIdx.y) / warpSize;\n",
        "    int globalWarpN = (blockIdx.x * blockDim.x + threadIdx.x);\n",
        "\n",
        "    for (int i = 0; i < n; i += WMMA_K) {\n",
        "        int aRow = globalWarpM * WMMA_M;\n",
        "        int aCol = i;\n",
        "        int bRow = i;\n",
        "        int bCol = globalWarpN * WMMA_N;\n",
        "        if (aRow < n && aCol < n && bRow < n && bCol < n) {\n",
        "            wmma::load_matrix_sync(a_frag, A + aRow * n + aCol, n);\n",
        "            wmma::load_matrix_sync(b_frag, B + bCol * n + bRow, n);\n",
        "            wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);\n",
        "        }\n",
        "    }\n",
        "    int cRow = globalWarpM * WMMA_M;\n",
        "    int cCol = globalWarpN * WMMA_N;\n",
        "    if (cRow < n && cCol < n) {\n",
        "        wmma::store_matrix_sync(C + cRow * n + cCol, c_frag, n, wmma::mem_row_major);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// MAIN RUNNER\n",
        "template <typename T>\n",
        "void run_complete_benchmark() {\n",
        "    size_t bytes = N * N * sizeof(T);\n",
        "\n",
        "    // --- 1. SETUP HOST DATA ---\n",
        "    cout << \"-------------------------Matrix Size: \" << N << \"x\" << N <<\"-------------------------\"<< endl ;\n",
        "    vector<T> h_A_vec = createMatrix<T>(N, N, 42);\n",
        "    vector<T> h_B_vec = createMatrix<T>(N, N, 43);\n",
        "    T *h_A = h_A_vec.data();\n",
        "    T *h_B = h_B_vec.data();\n",
        "\n",
        "    // --- 2. SETUP DEVICE DATA (Standard) ---\n",
        "    T *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, bytes);\n",
        "    cudaMalloc(&d_B, bytes);\n",
        "    cudaMalloc(&d_C, bytes);\n",
        "    cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start); cudaEventCreate(&stop);\n",
        "    float ms = 0;\n",
        "\n",
        "    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 grid((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    cout << \"\\n-----------------------------------------------------------\" << endl;\n",
        "    cout << \" BENCHMARK 1 & 2: STANDARD CORES (Config: \" << BLOCK_SIZE << \"x\" << BLOCK_SIZE << \" Tiles)\" << endl;\n",
        "    cout << \"-----------------------------------------------------------\" << endl;\n",
        "\n",
        "    // --- RUN ALGORITHM 1: NAIVE ---\n",
        "    cudaEventRecord(start);\n",
        "    matmul_naive<T><<<grid, block>>>(d_A, d_B, d_C, N);\n",
        "    cudaEventRecord(stop); cudaEventSynchronize(stop); cudaEventElapsedTime(&ms, start, stop);\n",
        "    printf(\">> 1. Naive (Global Mem):   %8.2f GFLOPS | Time: %.4f s\\n\", (2.0*N*N*N)/(ms/1000.0 * 1e9), ms/1000.0);\n",
        "\n",
        "    // --- RUN ALGORITHM 2: TILED ---\n",
        "    cudaEventRecord(start);\n",
        "    matmul_tiled<T><<<grid, block>>>(d_A, d_B, d_C, N);\n",
        "    cudaEventRecord(stop); cudaEventSynchronize(stop); cudaEventElapsedTime(&ms, start, stop);\n",
        "    printf(\">> 2. Tiled (Shared Mem):   %8.2f GFLOPS | Time: %.4f s\\n\", (2.0*N*N*N)/(ms/1000.0 * 1e9), ms/1000.0);\n",
        "\n",
        "    // --- 3. SETUP TENSOR CORE DATA (Reference) ---\n",
        "    cout << \"\\n-----------------------------------------------------------\" << endl;\n",
        "    cout << \" BENCHMARK 3: TENSOR CORES (Reference Speed)\" << endl;\n",
        "    cout << \"-----------------------------------------------------------\" << endl;\n",
        "\n",
        "    // We convert your chosen DATA_TYPE to Half Precision so Tensor Cores can process it\n",
        "    vector<half> h_A_half = convertToHalf(h_A_vec);\n",
        "    vector<half> h_B_half = convertToHalf(h_B_vec);\n",
        "\n",
        "    half *d_A_h, *d_B_h; float *d_C_f;\n",
        "    cudaMalloc(&d_A_h, N * N * sizeof(half));\n",
        "    cudaMalloc(&d_B_h, N * N * sizeof(half));\n",
        "    cudaMalloc(&d_C_f, N * N * sizeof(float));\n",
        "    cudaMemcpy(d_A_h, h_A_half.data(), N*N*sizeof(half), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B_h, h_B_half.data(), N*N*sizeof(half), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 grid_tc(N / 16, N / 16);\n",
        "    dim3 block_tc(32, 1, 1);\n",
        "\n",
        "    // --- RUN ALGORITHM 3: TENSOR CORES ---\n",
        "    cudaEventRecord(start);\n",
        "    matmul_tensor_core<<<grid_tc, block_tc>>>(d_A_h, d_B_h, d_C_f, N);\n",
        "    cudaEventRecord(stop); cudaEventSynchronize(stop); cudaEventElapsedTime(&ms, start, stop);\n",
        "    printf(\">> 3. Tensor Cores (WMMA):  %8.2f GFLOPS | Time: %.4f s\\n\", (2.0*N*N*N)/(ms/1000.0 * 1e9), ms/1000.0);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    cudaFree(d_A_h); cudaFree(d_B_h); cudaFree(d_C_f);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // This macro ensures we only instantiate the code for the type you selected\n",
        "    run_complete_benchmark<DATA_TYPE>();\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 matmul_gpu.cu -o matmul_gpu"
      ],
      "metadata": {
        "id": "yN8UCsGUDQk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmul_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWgkcAS8Ds00",
        "outputId": "a42d4e63-e7ba-4b15-c3ca-131009bca743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------Matrix Size: 8192x8192-------------------------\n",
            "\n",
            "-----------------------------------------------------------\n",
            " BENCHMARK 1 & 2: STANDARD CORES (Config: 32x32 Tiles)\n",
            "-----------------------------------------------------------\n",
            ">> 1. Naive (Global Mem):     574.15 GFLOPS | Time: 1.9150 s\n",
            ">> 2. Tiled (Shared Mem):     896.38 GFLOPS | Time: 1.2266 s\n",
            "\n",
            "-----------------------------------------------------------\n",
            " BENCHMARK 3: TENSOR CORES (Reference Speed)\n",
            "-----------------------------------------------------------\n",
            ">> 3. Tensor Cores (WMMA):  12377.34 GFLOPS | Time: 0.0888 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcWVKH8lDxmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}